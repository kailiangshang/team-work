#!/usr/bin/env python3
"""
å›¾è°±åˆå§‹åŒ–æµç¨‹ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ twork æ ¸å¿ƒåº“å®Œæˆä»æ–‡æ¡£åˆ°å›¾è°±çš„å®Œæ•´åˆå§‹åŒ–æµç¨‹ã€‚
"""

import json
import sys
from pathlib import Path

# æ·»åŠ  twork åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from twork.parser import (
    DocumentLoader,
    RequirementExtractor,
    DomainClassifier,
    WBSDecomposer
)
from twork.agent import RoleGenerator
from twork.generator import GraphBuilder
from twork.llm import OpenAIAdapter
from twork.utils.logger import get_logger

logger = get_logger("graph_init_demo")


def main():
    """ä¸»æµç¨‹"""
    
    print("=" * 80)
    print("twork å›¾è°±åˆå§‹åŒ–æµç¨‹ç¤ºä¾‹")
    print("=" * 80)
    
    # ===== æ­¥éª¤ 0: åˆå§‹åŒ– LLM =====
    print("\nğŸ“Œ æ­¥éª¤ 0: åˆå§‹åŒ– LLM")
    llm = OpenAIAdapter(api_key="your-openai-api-key")  # è¯·æ›¿æ¢ä¸ºå®é™…çš„ API Key
    print("âœ… LLM åˆå§‹åŒ–å®Œæˆ")
    
    # ===== æ­¥éª¤ 1: åŠ è½½æ–‡æ¡£ =====
    print("\nğŸ“Œ æ­¥éª¤ 1: åŠ è½½é¡¹ç›®æ–‡æ¡£")
    
    # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£(å®é™…ä½¿ç”¨æ—¶æ›¿æ¢ä¸ºçœŸå®æ–‡æ¡£è·¯å¾„)
    sample_doc_path = Path(__file__).parent / "sample_project.txt"
    
    if not sample_doc_path.exists():
        # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£
        sample_content = """
é¡¹ç›®åç§°: æ™ºèƒ½å®¢æœç³»ç»Ÿ

é¡¹ç›®æè¿°:
å¼€å‘ä¸€ä¸ªåŸºäº AI çš„æ™ºèƒ½å®¢æœç³»ç»Ÿ,æ”¯æŒå¤šæ¸ é“æ¥å…¥ã€æ™ºèƒ½é—®ç­”ã€å·¥å•ç®¡ç†ç­‰åŠŸèƒ½ã€‚

ä¸»è¦ç›®æ ‡:
1. æå‡å®¢æˆ·æœåŠ¡æ•ˆç‡,å‡å°‘äººå·¥å®¢æœå·¥ä½œé‡
2. æä¾› 7x24 å°æ—¶åœ¨çº¿æœåŠ¡
3. æ”¯æŒå¤šè¯­è¨€ã€å¤šæ¸ é“(ç½‘é¡µã€å¾®ä¿¡ã€APP)

æ ¸å¿ƒéœ€æ±‚:
1. ç”¨æˆ·ç®¡ç†:æ”¯æŒç”¨æˆ·æ³¨å†Œã€ç™»å½•ã€æƒé™ç®¡ç†
2. æ™ºèƒ½é—®ç­”:åŸºäº NLP çš„æ™ºèƒ½é—®ç­”å¼•æ“
3. å·¥å•ç³»ç»Ÿ:é—®é¢˜å·¥å•åˆ›å»ºã€åˆ†é…ã€è·Ÿè¸ª
4. çŸ¥è¯†åº“:æ”¯æŒçŸ¥è¯†åº“ç®¡ç†å’Œæ™ºèƒ½æ£€ç´¢
5. æ•°æ®åˆ†æ:å®¢æœæ•°æ®ç»Ÿè®¡å’Œåˆ†ææŠ¥è¡¨
6. ç³»ç»Ÿå¯¹æ¥:ä¸ç°æœ‰ CRM ç³»ç»Ÿå¯¹æ¥

çº¦æŸæ¡ä»¶:
- éœ€è¦å…¼å®¹ç°æœ‰ç³»ç»Ÿæ¶æ„
- æ•°æ®å®‰å…¨åˆè§„
- æ€§èƒ½è¦æ±‚:æ”¯æŒ 1000 å¹¶å‘ç”¨æˆ·

æœŸæœ›äº¤ä»˜ç‰©:
- Web ç®¡ç†åå°
- ç§»åŠ¨ç«¯ APP
- API æ¥å£æ–‡æ¡£
- ç”¨æˆ·ä½¿ç”¨æ‰‹å†Œ
"""
        sample_doc_path.write_text(sample_content, encoding="utf-8")
        print(f"âœ… åˆ›å»ºç¤ºä¾‹æ–‡æ¡£: {sample_doc_path}")
    
    loader = DocumentLoader()
    doc_result = loader.load(file_path=str(sample_doc_path))
    
    print(f"âœ… æ–‡æ¡£åŠ è½½æˆåŠŸ:")
    print(f"   - æ–‡ä»¶å: {doc_result['file_name']}")
    print(f"   - æ–‡ä»¶ç±»å‹: {doc_result['file_type']}")
    print(f"   - æ–‡ä»¶å¤§å°: {doc_result['file_size']} å­—èŠ‚")
    print(f"   - å†…å®¹é•¿åº¦: {len(doc_result['content'])} å­—ç¬¦")
    
    # ===== æ­¥éª¤ 2: æå–éœ€æ±‚ =====
    print("\nğŸ“Œ æ­¥éª¤ 2: æ™ºèƒ½æå–éœ€æ±‚ä¿¡æ¯")
    
    extractor = RequirementExtractor(llm_adapter=llm)
    requirements = extractor.extract(document_content=doc_result["content"])
    
    print(f"âœ… éœ€æ±‚æå–æˆåŠŸ:")
    print(f"   - é¡¹ç›®åç§°: {requirements.get('project_name', 'N/A')}")
    print(f"   - ä¸»è¦ç›®æ ‡: {len(requirements.get('main_objectives', []))} ä¸ª")
    print(f"   - æ ¸å¿ƒéœ€æ±‚: {len(requirements.get('key_requirements', []))} ä¸ª")
    print(f"   - çº¦æŸæ¡ä»¶: {len(requirements.get('constraints', []))} ä¸ª")
    
    # ===== æ­¥éª¤ 3: é¢†åŸŸåˆ†ç±» =====
    print("\nğŸ“Œ æ­¥éª¤ 3: é¡¹ç›®é¢†åŸŸåˆ†ç±»")
    
    classifier = DomainClassifier()
    domain_result = classifier.classify(
        content=requirements["project_description"],
        user_selected_domain=None  # å¯ä»¥æ‰‹åŠ¨æŒ‡å®šé¢†åŸŸ,å¦‚ "è½¯ä»¶å¼€å‘"
    )
    
    print(f"âœ… é¢†åŸŸåˆ†ç±»å®Œæˆ:")
    print(f"   - é¢†åŸŸç±»å‹: {domain_result['domain_type']}")
    print(f"   - ç½®ä¿¡åº¦: {domain_result['confidence']}")
    print(f"   - å…³é”®è¯: {', '.join(domain_result['keywords'][:5])}...")
    print(f"   - æ¨¡æ¿ID: {domain_result['template_id']}")
    
    # ç”¨æˆ·å¯åœ¨æ­¤å¤„ä¿®æ”¹é¢†åŸŸ(å¯ç¼–è¾‘ç‚¹)
    # domain_result['domain_type'] = "è½¯ä»¶å¼€å‘"  # æ‰‹åŠ¨ä¿®æ”¹
    
    # ===== æ­¥éª¤ 4: WBS ä»»åŠ¡æ‹†è§£ â­ æ ¸å¿ƒå¯ç¼–è¾‘ç‚¹ =====
    print("\nğŸ“Œ æ­¥éª¤ 4: WBS ä»»åŠ¡æ‹†è§£")
    
    decomposer = WBSDecomposer(llm_adapter=llm, max_level=4)
    wbs_result = decomposer.decompose(
        requirements=json.dumps(requirements, ensure_ascii=False),
        domain_type=domain_result["domain_type"],
        task_types=["éœ€æ±‚åˆ†æ", "è®¾è®¡", "å¼€å‘", "æµ‹è¯•", "éƒ¨ç½²"],
        template_config={},
        user_constraints={
            "total_days": 60,
            "team_size": 8
        }
    )
    
    task_tree = wbs_result["task_tree"]
    stats = wbs_result["statistics"]
    
    print(f"âœ… WBS æ‹†è§£å®Œæˆ:")
    print(f"   - æ€»ä»»åŠ¡æ•°: {stats['total_tasks']}")
    print(f"   - æœ€å¤§å±‚çº§: {stats['max_level_reached']}")
    print(f"   - å¹³å‡å¤æ‚åº¦: {stats['avg_complexity']}")
    print(f"   - å„å±‚çº§ä»»åŠ¡æ•°: {stats['tasks_by_level']}")
    
    # éªŒè¯ä¾èµ–å…³ç³»(æ£€æµ‹å¾ªç¯ä¾èµ–)
    print("\nğŸ“Œ æ­¥éª¤ 4.1: éªŒè¯ä»»åŠ¡ä¾èµ–å…³ç³»")
    is_valid, errors = decomposer.validate_dependencies(task_tree)
    
    if is_valid:
        print("âœ… ä¾èµ–å…³ç³»éªŒè¯é€šè¿‡,æ— å¾ªç¯ä¾èµ–")
    else:
        print("âŒ ä¾èµ–å…³ç³»éªŒè¯å¤±è´¥:")
        for error in errors:
            print(f"   - {error}")
    
    # ç”¨æˆ·å¯åœ¨æ­¤å¤„ç¼–è¾‘ä»»åŠ¡æ ‘(å¯ç¼–è¾‘ç‚¹)
    # ç¤ºä¾‹:åˆ é™¤ä»»åŠ¡ã€ä¿®æ”¹ä»»åŠ¡å±æ€§ã€æ·»åŠ ä»»åŠ¡ç­‰
    # task_tree[0]["estimated_complexity"] = 8  # ä¿®æ”¹å¤æ‚åº¦
    # task_tree[0]["children"].pop()  # åˆ é™¤å­ä»»åŠ¡
    
    # ===== æ­¥éª¤ 5: ç”Ÿæˆè§’è‰² Agent â­ æ ¸å¿ƒå¯ç¼–è¾‘ç‚¹ =====
    print("\nğŸ“Œ æ­¥éª¤ 5: ç”Ÿæˆé¡¹ç›®å›¢é˜Ÿè§’è‰²")
    
    generator = RoleGenerator(llm_adapter=llm)
    agents = generator.generate_roles(
        task_tree=task_tree,
        domain_type=domain_result["domain_type"],
        team_size_hint=8
    )
    
    print(f"âœ… è§’è‰²ç”Ÿæˆå®Œæˆ: å…± {len(agents)} ä¸ªè§’è‰²")
    for i, agent in enumerate(agents[:3], 1):  # æ˜¾ç¤ºå‰3ä¸ªè§’è‰²
        print(f"   {i}. {agent['role_name']} ({agent['role_type']})")
        print(f"      - ID: {agent['agent_id']}")
        print(f"      - æŠ€èƒ½: {len(agent.get('capabilities', []))} é¡¹")
        print(f"      - åˆ†é…ä»»åŠ¡: {len(agent.get('assigned_tasks', []))} ä¸ª")
    
    if len(agents) > 3:
        print(f"   ... è¿˜æœ‰ {len(agents) - 3} ä¸ªè§’è‰²")
    
    # ç”¨æˆ·å¯åœ¨æ­¤å¤„ç¼–è¾‘ Agent(å¯ç¼–è¾‘ç‚¹)
    # ç¤ºä¾‹:åˆ é™¤ Agentã€ä¿®æ”¹æŠ€èƒ½ã€é‡æ–°åˆ†é…ä»»åŠ¡
    
    # åˆ é™¤ Agent ç¤ºä¾‹
    # removed_agent = agents.pop(1)  # åˆ é™¤ç¬¬2ä¸ª Agent
    # orphan_tasks = removed_agent.get("assigned_tasks", [])
    # 
    # # é‡æ–°åˆ†é…å­¤å„¿ä»»åŠ¡
    # agents = generator.reassign_tasks(
    #     agents=agents,
    #     task_tree=task_tree,
    #     orphan_tasks=orphan_tasks
    # )
    
    # è·å–ä»»åŠ¡åˆ†é…æ¨è
    print("\nğŸ“Œ æ­¥éª¤ 5.1: ç”Ÿæˆä»»åŠ¡åˆ†é…æ¨è")
    recommendations = generator.recommend_assignments(
        agents=agents,
        task_tree=task_tree,
        strategy="skill_match"  # æˆ– "workload_balance"
    )
    
    print(f"âœ… æ¨èåˆ†é…: {len(recommendations)} ä¸ªä»»åŠ¡")
    print(f"   ç¤ºä¾‹æ¨è: {list(recommendations.items())[:3]}")
    
    # ===== æ­¥éª¤ 6: æ„å»ºå›¾è°± =====
    print("\nğŸ“Œ æ­¥éª¤ 6: æ„å»ºçŸ¥è¯†å›¾è°±")
    
    builder = GraphBuilder()
    
    # å±•å¹³ä»»åŠ¡æ ‘
    flat_tasks = decomposer.flatten_tree(task_tree)
    print(f"   - å±•å¹³ä»»åŠ¡æ•°: {len(flat_tasks)}")
    
    # æ„å»ºä¸‰å…ƒç»„
    triplets = builder.build_triplets(tasks=flat_tasks, agents=agents)
    print(f"âœ… ä¸‰å…ƒç»„æ„å»ºå®Œæˆ: {len(triplets)} ä¸ªä¸‰å…ƒç»„")
    
    # ç¤ºä¾‹ä¸‰å…ƒç»„
    print(f"   ç¤ºä¾‹ä¸‰å…ƒç»„:")
    for triplet in triplets[:5]:
        print(f"   - {triplet}")
    
    # ===== æ­¥éª¤ 7: å¯¼å‡ºç»“æœ =====
    print("\nğŸ“Œ æ­¥éª¤ 7: å¯¼å‡ºç»“æœæ–‡ä»¶")
    
    output_dir = Path(__file__).parent / "output"
    output_dir.mkdir(exist_ok=True)
    
    # å¯¼å‡ºä¸‰å…ƒç»„
    triplets_file = output_dir / "graph_triplets.json"
    builder.export_triplets(triplets, str(triplets_file))
    print(f"âœ… ä¸‰å…ƒç»„å·²å¯¼å‡º: {triplets_file}")
    
    # å¯¼å‡º Mermaid å›¾è°±
    mermaid_file = output_dir / "graph.md"
    builder.export_mermaid(flat_tasks, agents, str(mermaid_file))
    print(f"âœ… Mermaid å›¾è°±å·²å¯¼å‡º: {mermaid_file}")
    
    # å¯¼å‡ºä»»åŠ¡æ ‘
    task_tree_file = output_dir / "task_tree.json"
    task_tree_file.write_text(
        json.dumps(task_tree, ensure_ascii=False, indent=2),
        encoding="utf-8"
    )
    print(f"âœ… ä»»åŠ¡æ ‘å·²å¯¼å‡º: {task_tree_file}")
    
    # å¯¼å‡º Agent é…ç½®
    agents_file = output_dir / "agents.json"
    agents_file.write_text(
        json.dumps(agents, ensure_ascii=False, indent=2),
        encoding="utf-8"
    )
    print(f"âœ… Agent é…ç½®å·²å¯¼å‡º: {agents_file}")
    
    # ===== å®Œæˆ =====
    print("\n" + "=" * 80)
    print("ğŸ‰ å›¾è°±åˆå§‹åŒ–æµç¨‹å®Œæˆ!")
    print("=" * 80)
    print(f"\nè¾“å‡ºæ–‡ä»¶ç›®å½•: {output_dir}")
    print("\nå¯ç¼–è¾‘ç‚¹æ€»ç»“:")
    print("1. æ­¥éª¤ 3: å¯æ‰‹åŠ¨ä¿®æ”¹é¢†åŸŸåˆ†ç±»")
    print("2. æ­¥éª¤ 4: å¯ç¼–è¾‘ä»»åŠ¡æ ‘(åˆ é™¤/ä¿®æ”¹/æ·»åŠ ä»»åŠ¡)")
    print("3. æ­¥éª¤ 5: å¯ç¼–è¾‘ Agent(åˆ é™¤/ä¿®æ”¹æŠ€èƒ½/é‡æ–°åˆ†é…ä»»åŠ¡)")
    print("\nä¸‹ä¸€æ­¥:")
    print("- å°†æ•°æ®å¯¼å…¥å¤–éƒ¨ç³»ç»Ÿ(å¦‚ FastAPI åç«¯)")
    print("- ä½¿ç”¨ Neo4j å­˜å‚¨å›¾è°±ä¸‰å…ƒç»„")
    print("- è¿è¡Œæ¨¡æ‹Ÿå¼•æ“æ‰§è¡Œä»»åŠ¡")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logger.error(f"ç¤ºä¾‹è¿è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        sys.exit(1)
