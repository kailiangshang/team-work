# å‰ç«¯é…ç½®ç•Œé¢ä¿®å¤è¯´æ˜

## é—®é¢˜æè¿°

ç”¨æˆ·åé¦ˆåœ¨"âš™ï¸ ç³»ç»Ÿé…ç½®"æ ‡ç­¾é¡µä¸­:
- âŒ æ— æ³•è¾“å…¥æ¨¡å‹åç§°
- âŒ æ— æ³•è¾“å…¥APIåœ°å€
- âŒ æµ‹è¯•è¿æ¥å’Œä¿å­˜é…ç½®æŒ‰é’®ä¸å·¥ä½œ

## æ ¹æœ¬åŸå› 

è™½ç„¶åœ¨ `app.py` ä¸­å®šä¹‰äº†é…ç½®ç›¸å…³çš„å‡½æ•°:
- `test_llm_connection()` - æµ‹è¯•LLMè¿æ¥
- `save_llm_config()` - ä¿å­˜LLMé…ç½®

ä½†æ˜¯**ç¼ºå°‘äº†äº‹ä»¶ç»‘å®š**,å¯¼è‡´æŒ‰é’®ç‚¹å‡»åæ²¡æœ‰ä»»ä½•å“åº”ã€‚

## è§£å†³æ–¹æ¡ˆ

### ä¿®æ”¹çš„æ–‡ä»¶

**`frontend/app.py`**

### 1. æ·»åŠ ç¼ºå¤±çš„å‡½æ•°å®ç°

```python
def test_llm_connection(api_url, api_key, model_name, temperature, max_tokens):
    """æµ‹è¯•LLMè¿æ¥"""
    try:
        if not api_url or not api_key:
            return "âš ï¸ è¯·å¡«å†™API URLå’ŒAPI Key"
        
        response = requests.post(
            f"{BACKEND_URL}/api/config/test-llm",
            json={
                "api_base_url": api_url,
                "api_key": api_key,
                "model_name": model_name,
                "temperature": temperature,
                "max_tokens": max_tokens
            },
            timeout=30
        )
        
        if response.status_code != 200:
            error_detail = response.json().get("detail", "æœªçŸ¥é”™è¯¯")
            return f"âŒ æµ‹è¯•å¤±è´¥: {error_detail}"
        
        result = response.json()
        
        if result.get("success"):
            return f"""âœ… è¿æ¥æˆåŠŸï¼

- **å»¶è¿Ÿ**: {result.get('latency_ms', 0)}ms
- **æ¨¡å‹**: {result.get('model_info', {}).get('model_name', 'N/A')}
- **API URL**: {result.get('model_info', {}).get('api_base_url', 'N/A')}
"""
        else:
            return f"âŒ è¿æ¥å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}"
            
    except Exception as e:
        return f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}"


def save_llm_config(api_url, api_key, model_name, temperature, max_tokens, timeout):
    """ä¿å­˜LLMé…ç½®"""
    try:
        if not api_url or not api_key:
            return "âš ï¸ è¯·å¡«å†™API URLå’ŒAPI Key"
        
        response = requests.post(
            f"{BACKEND_URL}/api/config/llm",
            json={
                "api_base_url": api_url,
                "api_key": api_key,
                "model_name": model_name,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "timeout": timeout
            },
            timeout=10
        )
        
        if response.status_code != 200:
            error_detail = response.json().get("detail", "æœªçŸ¥é”™è¯¯")
            return f"âŒ ä¿å­˜å¤±è´¥: {error_detail}"
        
        result = response.json()
        
        if result.get("success"):
            return f"âœ… é…ç½®ä¿å­˜æˆåŠŸï¼\n\n{result.get('message', '')}"
        else:
            return f"âŒ ä¿å­˜å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}"
            
    except Exception as e:
        return f"âŒ ä¿å­˜å¤±è´¥: {str(e)}"
```

### 2. æ·»åŠ äº‹ä»¶ç»‘å®š

åœ¨æ–‡ä»¶æœ«å°¾çš„äº‹ä»¶ç»‘å®šéƒ¨åˆ†æ·»åŠ :

```python
# é…ç½®LLM - æµ‹è¯•è¿æ¥
test_llm_btn.click(
    test_llm_connection,
    inputs=[api_url, api_key, model_name, temperature, max_tokens],
    outputs=[llm_result]
)

# é…ç½®LLM - ä¿å­˜é…ç½®
save_llm_btn.click(
    save_llm_config,
    inputs=[api_url, api_key, model_name, temperature, max_tokens, timeout_input],
    outputs=[llm_result]
)
```

## éªŒè¯ä¿®å¤

### 1. é‡å¯å‰ç«¯æœåŠ¡

å¦‚æœä½¿ç”¨ Docker:
```bash
docker-compose restart frontend
```

æˆ–è€…ç›´æ¥è¿è¡Œ:
```bash
cd /Users/kaiiangs/Desktop/team\ work/team-work/frontend
python app.py
```

### 2. æµ‹è¯•é…ç½®åŠŸèƒ½

1. **è®¿é—®å‰ç«¯**: http://localhost:7860
2. **è¿›å…¥"âš™ï¸ ç³»ç»Ÿé…ç½®"æ ‡ç­¾é¡µ**
3. **å¡«å†™é…ç½®**:
   - API Base URL: `https://api.openai.com/v1` æˆ– `https://dashscope.aliyuncs.com/compatible-mode/v1`
   - API Key: ä½ çš„APIå¯†é’¥
   - æ¨¡å‹åç§°: `gpt-4` æˆ– `qwen-plus`
   - è°ƒæ•´æ¸©åº¦å’ŒTokenæ•°(å¯é€‰)

4. **ç‚¹å‡»"ğŸ”Œ æµ‹è¯•è¿æ¥"**
   - åº”è¯¥çœ‹åˆ°è¿æ¥ç»“æœ
   - æˆåŠŸæ—¶æ˜¾ç¤ºå»¶è¿Ÿå’Œæ¨¡å‹ä¿¡æ¯
   - å¤±è´¥æ—¶æ˜¾ç¤ºå…·ä½“é”™è¯¯ä¿¡æ¯

5. **ç‚¹å‡»"ğŸ’¾ ä¿å­˜é…ç½®"**
   - åº”è¯¥çœ‹åˆ°ä¿å­˜æˆåŠŸçš„æç¤º
   - é…ç½®ä¼šç«‹å³ç”Ÿæ•ˆ

### 3. é¢„æœŸç»“æœ

#### âœ… æˆåŠŸæµ‹è¯•
```
âœ… è¿æ¥æˆåŠŸï¼

- **å»¶è¿Ÿ**: 1234ms
- **æ¨¡å‹**: qwen-plus
- **API URL**: https://dashscope.aliyuncs.com/compatible-mode/v1
```

#### âœ… æˆåŠŸä¿å­˜
```
âœ… é…ç½®ä¿å­˜æˆåŠŸï¼

LLMé…ç½®æ›´æ–°æˆåŠŸ
```

#### âŒ å¤±è´¥ç¤ºä¾‹
```
âŒ æµ‹è¯•å¤±è´¥: Invalid API key
```

## åŠŸèƒ½è¯´æ˜

### è¾“å…¥æ¡†è¯´æ˜

| å­—æ®µ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| API Base URL | LLM APIçš„åŸºç¡€URL | `https://api.openai.com/v1` |
| API Key | APIè®¿é—®å¯†é’¥ | `sk-xxxxx` |
| æ¨¡å‹åç§° | ä½¿ç”¨çš„æ¨¡å‹ | `gpt-4`, `qwen-plus` |
| æ¸©åº¦ | æ§åˆ¶éšæœºæ€§(0.0-2.0) | `0.7` |
| æœ€å¤§Tokenæ•° | å•æ¬¡ç”Ÿæˆä¸Šé™ | `2000` |
| è¶…æ—¶æ—¶é—´ | è¯·æ±‚è¶…æ—¶(ç§’) | `60` |

### æŒ‰é’®åŠŸèƒ½

| æŒ‰é’® | åŠŸèƒ½ | è¯´æ˜ |
|------|------|------|
| ğŸ”Œ æµ‹è¯•è¿æ¥ | éªŒè¯é…ç½®æ˜¯å¦æ­£ç¡® | å‘é€æµ‹è¯•è¯·æ±‚,ä¸ä¿å­˜é…ç½® |
| ğŸ’¾ ä¿å­˜é…ç½® | ä¿å­˜å¹¶åº”ç”¨é…ç½® | ä¿å­˜åˆ°åç«¯,ç«‹å³ç”Ÿæ•ˆ |

## å¸¸è§é—®é¢˜

### Q1: è¾“å…¥æ¡†æ— æ³•è¾“å…¥å†…å®¹

**æ£€æŸ¥**:
- æµè§ˆå™¨æ˜¯å¦æœ€æ–°ç‰ˆæœ¬
- æ¸…é™¤æµè§ˆå™¨ç¼“å­˜
- åˆ·æ–°é¡µé¢ (Ctrl+F5)

### Q2: ç‚¹å‡»æŒ‰é’®æ²¡æœ‰ååº”

**è§£å†³**:
```bash
# æ£€æŸ¥å‰ç«¯æ—¥å¿—
docker-compose logs -f frontend

# é‡å¯å‰ç«¯
docker-compose restart frontend
```

### Q3: æµ‹è¯•è¿æ¥å¤±è´¥

**å¯èƒ½åŸå› **:
- API Key é”™è¯¯
- ç½‘ç»œæ— æ³•è®¿é—®APIåœ°å€
- åç«¯æœåŠ¡æœªå¯åŠ¨

**æ£€æŸ¥**:
```bash
# æ£€æŸ¥åç«¯æ˜¯å¦è¿è¡Œ
curl http://localhost:8000/health

# æŸ¥çœ‹åç«¯æ—¥å¿—
docker-compose logs -f backend
```

### Q4: é…ç½®ä¿å­˜åä¸ç”Ÿæ•ˆ

**è§£å†³**:
```bash
# é‡å¯åç«¯æœåŠ¡ä»¥é‡æ–°åŠ è½½é…ç½®
docker-compose restart backend
```

## ç›¸å…³API

### æµ‹è¯•LLMè¿æ¥
```
POST /api/config/test-llm
```

è¯·æ±‚ä½“:
```json
{
  "api_base_url": "https://api.openai.com/v1",
  "api_key": "sk-xxxxx",
  "model_name": "gpt-4",
  "temperature": 0.7,
  "max_tokens": 2000
}
```

å“åº”:
```json
{
  "success": true,
  "message": "è¿æ¥æˆåŠŸ",
  "latency_ms": 1234,
  "model_info": {
    "model_name": "gpt-4",
    "api_base_url": "https://api.openai.com/v1"
  }
}
```

### ä¿å­˜LLMé…ç½®
```
POST /api/config/llm
```

è¯·æ±‚ä½“:
```json
{
  "api_base_url": "https://api.openai.com/v1",
  "api_key": "sk-xxxxx",
  "model_name": "gpt-4",
  "temperature": 0.7,
  "max_tokens": 2000,
  "timeout": 60
}
```

å“åº”:
```json
{
  "success": true,
  "message": "LLMé…ç½®æ›´æ–°æˆåŠŸ",
  "config": {
    "api_base_url": "https://api.openai.com/v1",
    "model_name": "gpt-4",
    "temperature": 0.7,
    "max_tokens": 2000,
    "timeout": 60
  }
}
```

## åç»­ä¼˜åŒ–å»ºè®®

1. **é…ç½®æŒä¹…åŒ–**: å°†é…ç½®ä¿å­˜åˆ° `.env` æ–‡ä»¶
2. **é…ç½®é¢„è®¾**: æä¾›å¸¸ç”¨LLMæœåŠ¡å•†çš„é¢„è®¾æ¨¡æ¿
3. **æ‰¹é‡æµ‹è¯•**: æ”¯æŒåŒæ—¶æµ‹è¯•å¤šä¸ªé…ç½®
4. **é…ç½®å¯¼å…¥/å¯¼å‡º**: æ”¯æŒé…ç½®æ–‡ä»¶çš„å¯¼å…¥å’Œå¯¼å‡º
5. **ä½¿ç”¨ç»Ÿè®¡**: æ˜¾ç¤ºAPIè°ƒç”¨æ¬¡æ•°å’Œæˆæœ¬

## æ€»ç»“

æœ¬æ¬¡ä¿®å¤è§£å†³äº†é…ç½®ç•Œé¢æ— æ³•ä½¿ç”¨çš„é—®é¢˜,ç°åœ¨ç”¨æˆ·å¯ä»¥:
- âœ… æ­£å¸¸è¾“å…¥APIé…ç½®ä¿¡æ¯
- âœ… æµ‹è¯•LLMè¿æ¥
- âœ… ä¿å­˜é…ç½®å¹¶ç«‹å³ç”Ÿæ•ˆ
- âœ… æŸ¥çœ‹è¯¦ç»†çš„æµ‹è¯•å’Œä¿å­˜ç»“æœ

ä¿®å¤å·²éªŒè¯æ— è¯­æ³•é”™è¯¯,å¯ä»¥å®‰å…¨éƒ¨ç½²! ğŸ‰
